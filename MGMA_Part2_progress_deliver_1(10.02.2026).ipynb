{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "805ffdb94dab4d67bf1548c69247e763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_608d88cca38e4f4ca649723bfd8d329f",
              "IPY_MODEL_65eb45f75fae403ea190e24105774804",
              "IPY_MODEL_e61c0dcfad474c8b8081aeb45de316f5"
            ],
            "layout": "IPY_MODEL_2af4cd9a63f14f40998a86b3b8a7e2da"
          }
        },
        "608d88cca38e4f4ca649723bfd8d329f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5cc6468dbea4aae8c3bc31c6ee38397",
            "placeholder": "​",
            "style": "IPY_MODEL_591713b699e94ce4ab8e79be8323429b",
            "value": "Generating walks: 100%"
          }
        },
        "65eb45f75fae403ea190e24105774804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec30572804b4cbabdfae748b9e6b01a",
            "max": 3348630,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03353b9fd753450ba56f96f35c2e774c",
            "value": 3348630
          }
        },
        "e61c0dcfad474c8b8081aeb45de316f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_081783c96eea426d9d320d71e9a937d8",
            "placeholder": "​",
            "style": "IPY_MODEL_1f4158da7a5d408b9dcb24f17ff19128",
            "value": " 3348630/3348630 [03:53&lt;00:00, 17041.38it/s]"
          }
        },
        "2af4cd9a63f14f40998a86b3b8a7e2da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5cc6468dbea4aae8c3bc31c6ee38397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "591713b699e94ce4ab8e79be8323429b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ec30572804b4cbabdfae748b9e6b01a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03353b9fd753450ba56f96f35c2e774c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "081783c96eea426d9d320d71e9a937d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4158da7a5d408b9dcb24f17ff19128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Embeddings, Node Classification & Link Prediction\n",
        "### Semester 2 Extension — Massive Graph Management and Analytics\n",
        "\n",
        "---\n",
        "\n",
        "**Author:** Olha Baliasina and Samuel Chapuis\n",
        "\n",
        "**Datasets:**\n",
        "- **Amazon Co-Purchasing Network** (~335K nodes, ~926K edges) — Large\n",
        "- **CA-HepTh Collaboration Network** (~6K–9K authors) — Small\n",
        "\n",
        "**This notebook covers:**\n",
        "1. Shallow Embeddings: DeepWalk, Node2Vec\n",
        "2. Spectral Embeddings: Laplacian Eigenmaps\n",
        "3. GNN-based: GCN, GraphSAGE, GAT\n",
        "4. Node Classification on both datasets\n",
        "5. Link Prediction on both datasets\n",
        "6. Comprehensive comparison & analysis"
      ],
      "metadata": {
        "id": "ASbGG-K1ojyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 0: Setup & Dependencies\n"
      ],
      "metadata": {
        "id": "Smd8oQY-omuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Pin NumPy to what torch wheels commonly expect, and reinstall compiled stack to match it\n",
        "%pip install -q --no-cache-dir --force-reinstall \\\n",
        "  numpy==1.26.4 pandas scipy scikit-learn\n",
        "\n",
        "# 2) Install PyTorch (CUDA 12.6)\n",
        "%pip install -q --no-cache-dir \\\n",
        "  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# 3) Install PyG compiled wheels (binary only, no source builds)\n",
        "%pip install -q --no-cache-dir --only-binary=:all: \\\n",
        "  pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv \\\n",
        "  -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
        "\n",
        "%pip install -q --no-cache-dir torch-geometric\n",
        "\n",
        "# 4) The remaining libraries you import/use\n",
        "%pip install -q --no-cache-dir umap-learn node2vec gensim python-louvain python-igraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfosYzrrLDQe",
        "outputId": "f6a1cd20-3728-4aa6-9e24-8b1bbc5c0a95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m229.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m178.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m175.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m214.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m147.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m346.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m244.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.0 which is incompatible.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.0 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.0 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.8/821.8 MB\u001b[0m \u001b[31m269.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m187.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m195.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m189.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m204.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m351.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m278.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m443.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/texttable/\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m297.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m367.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy\n",
        "# print(\"numpy:\", numpy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D95_oANrHjen",
        "outputId": "47de85d6-10f6-44bd-f64b-403b92077d4f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---- install PyTorch CUDA 12.6 wheels ----\n",
        "# %pip install -q --no-cache-dir torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# # ---- install PyG compiled wheels (BINARY ONLY: no source builds) ----\n",
        "# %pip install -q --no-cache-dir --only-binary=:all: pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
        "# %pip install -q torch-geometric\n",
        "\n",
        "# # ---- the remaining libs you use ----\n",
        "# %pip install -q node2vec gensim umap-learn python-louvain python-igraph\n"
      ],
      "metadata": {
        "id": "CQGJunJlG3Tl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # PyTorch (CUDA 12.6) + PyG wheels that match it (no source builds)\n",
        "# %pip install -q --no-cache-dir torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "# %pip install -q --no-cache-dir --only-binary=:all: pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv \\\n",
        "#   -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
        "# %pip install -q torch-geometric\n",
        "\n",
        "# # Your extra libs\n",
        "# %pip install -q umap-learn node2vec gensim python-louvain python-igraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hajZpXdI9V6",
        "outputId": "67e552af-8286-4cc9-92b9-c959ef002275"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.8/821.8 MB\u001b[0m \u001b[31m238.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m183.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m253.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m225.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m215.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m277.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m318.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m293.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m135.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np, pandas as pd\n",
        "# import torch, torch_geometric, umap\n",
        "# print(\"numpy\", np.__version__)\n",
        "# print(\"pandas\", pd.__version__)\n",
        "# print(\"torch\", torch.__version__, \"cuda\", torch.version.cuda)\n",
        "# print(\"pyg\", torch_geometric.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "PA_vCQuwJN1u",
        "outputId": "02154aa6-b56a-40b8-ad6b-e6debc85ead7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2584410184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -q torch-geometric\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn8uhhCj0zCg",
        "outputId": "886f882a-cc9e-4bfb-af51-d089622eb5ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVVijVXBLOKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ssjhY1RLOXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBT_LLZQLOiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -q node2vec gensim umap-learn python-louvain python-igraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjJte5y_H78c",
        "outputId": "24aecf6a-2098-4331-bbf4-ef6601b79335"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import re, torch\n",
        "\n",
        "# torch_ver = re.match(r\"(\\d+\\.\\d+\\.\\d+)\", torch.__version__).group(1)\n",
        "# cuda_ver = torch.version.cuda\n",
        "\n",
        "# if cuda_ver is None:\n",
        "#     cuda_tag = \"cpu\"\n",
        "# else:\n",
        "#     cuda_tag = \"cu\" + cuda_ver.replace(\".\", \"\")\n",
        "\n",
        "# url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
        "# print(\"Using wheels from:\", url)\n",
        "\n",
        "# %pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv -f {url}\n",
        "# %pip install -q torch-geometric\n"
      ],
      "metadata": {
        "id": "O8W4PXuky16y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -q node2vec gensim umap-learn\n",
        "# %pip install -q python-louvain python-igraph\n",
        "# %pip install -q numpy\n"
      ],
      "metadata": {
        "id": "am_GSth1y6B_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install torch-geometric\n",
        "# !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "# !pip install node2vec gensim umap-learn\n",
        "# !pip install community python-louvain igraph\n"
      ],
      "metadata": {
        "id": "gJQ18cWPoof3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgEHwueJohmD",
        "outputId": "0587920f-796e-4712-9412-b1eade4c3835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "PyTorch version: 2.8.0+cu126\n",
            "PyG version: 2.7.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0.2  Imports\n",
        "# ============================================================\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import pickle\n",
        "from collections import Counter, defaultdict, deque\n",
        "from itertools import combinations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from scipy import sparse\n",
        "from scipy.sparse.linalg import eigsh\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch Geometric\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import NeighborLoader, LinkNeighborLoader\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.utils import (\n",
        "    from_networkx, negative_sampling, train_test_split_edges,\n",
        "    to_undirected, add_self_loops\n",
        ")\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "# Gensim for shallow embeddings\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Node2Vec\n",
        "from node2vec import Node2Vec as Node2VecLib\n",
        "\n",
        "# UMAP for visualization\n",
        "import umap\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"PyG version: {torch_geometric.__version__}\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 1: Data Loading & Preprocessing\n",
        "\n",
        "We load both datasets and prepare them for the experiments.\n",
        "\n",
        "### Data files required:\n",
        "- `data/com-amazon.ungraph.txt` — Amazon edge list\n",
        "- `data/com-amazon.all.dedup.cmty.txt` — Amazon ground-truth communities\n",
        "- `arxiv_hepth_meta.csv` — HepTh paper metadata\n",
        "- (Optional) `data/CA-HepTh.txt` — HepTh original edge list"
      ],
      "metadata": {
        "id": "dWFxPbbco11A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Amazon Co-Purchasing Network"
      ],
      "metadata": {
        "id": "yuFgZZL_rVyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.1  Load Amazon graph\n",
        "# ============================================================\n",
        "GRAPH_FILE = 'data/com-amazon.ungraph.txt'\n",
        "COMMUNITY_FILE = 'data/com-amazon.all.dedup.cmty.txt'\n",
        "\n",
        "print(\"Loading Amazon co-purchasing network...\")\n",
        "t0 = time.time()\n",
        "G_amazon = nx.read_edgelist(\n",
        "    GRAPH_FILE, comments='#', delimiter='\\t',\n",
        "    create_using=nx.Graph(), nodetype=int\n",
        ")\n",
        "print(f\"Loaded in {time.time()-t0:.1f}s — {G_amazon.number_of_nodes():,} nodes, {G_amazon.number_of_edges():,} edges\")\n",
        "\n",
        "# Load ground-truth communities\n",
        "ground_truth_communities = []\n",
        "with open(COMMUNITY_FILE, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line and not line.startswith('#'):\n",
        "            members = [int(x) for x in line.split('\\t')]\n",
        "            ground_truth_communities.append(set(members))\n",
        "\n",
        "print(f\"Loaded {len(ground_truth_communities):,} ground-truth communities\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yudR2XObozxM",
        "outputId": "387dd02a-4390-4530-9b45-bfe516c4842f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Amazon co-purchasing network...\n",
            "Loaded in 6.5s — 334,863 nodes, 925,872 edges\n",
            "Loaded 75,149 ground-truth communities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.2  Construct node labels for Amazon (node classification)\n",
        "# ============================================================\n",
        "# Each node can belong to multiple communities (overlapping).\n",
        "# Strategy: assign each node the community it shares with the\n",
        "# MOST neighbors (i.e., the community most structurally relevant).\n",
        "# Then keep only the top-K most frequent labels.\n",
        "\n",
        "# Build node -> list of community IDs\n",
        "node_to_comms = defaultdict(list)\n",
        "for cid, comm in enumerate(ground_truth_communities):\n",
        "    for node in comm:\n",
        "        if node in G_amazon:\n",
        "            node_to_comms[node].append(cid)\n",
        "\n",
        "# For each node, pick the community that most of its neighbors also belong to\n",
        "def assign_best_community(G, node_to_comms):\n",
        "    labels = {}\n",
        "    for node in G.nodes():\n",
        "        if node not in node_to_comms or len(node_to_comms[node]) == 0:\n",
        "            continue\n",
        "        neighbors = set(G.neighbors(node))\n",
        "        best_cid, best_score = None, -1\n",
        "        for cid in node_to_comms[node]:\n",
        "            comm_set = ground_truth_communities[cid]\n",
        "            overlap = len(neighbors & comm_set)\n",
        "            if overlap > best_score:\n",
        "                best_score = overlap\n",
        "                best_cid = cid\n",
        "        if best_cid is not None:\n",
        "            labels[node] = best_cid\n",
        "    return labels\n",
        "\n",
        "print(\"Assigning primary community labels to nodes...\")\n",
        "t0 = time.time()\n",
        "amazon_node_labels = assign_best_community(G_amazon, node_to_comms)\n",
        "print(f\"Done in {time.time()-t0:.1f}s — {len(amazon_node_labels):,} nodes labeled\")\n",
        "\n",
        "# Keep top-K most frequent labels\n",
        "TOP_K = 20\n",
        "label_counts = Counter(amazon_node_labels.values())\n",
        "top_labels = set([l for l, _ in label_counts.most_common(TOP_K)])\n",
        "amazon_node_labels_filtered = {n: l for n, l in amazon_node_labels.items() if l in top_labels}\n",
        "\n",
        "# Re-encode labels to 0..K-1\n",
        "le_amazon = LabelEncoder()\n",
        "labeled_nodes_amazon = sorted(amazon_node_labels_filtered.keys())\n",
        "raw_labels = [amazon_node_labels_filtered[n] for n in labeled_nodes_amazon]\n",
        "encoded_labels_amazon = le_amazon.fit_transform(raw_labels)\n",
        "\n",
        "print(f\"\\nFiltered to top-{TOP_K} categories: {len(amazon_node_labels_filtered):,} labeled nodes\")\n",
        "print(f\"Label distribution:\")\n",
        "for label_id, count in sorted(Counter(encoded_labels_amazon).items(), key=lambda x: -x[1])[:10]:\n",
        "    print(f\"  Class {label_id}: {count:,} nodes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHDIQOpmuhjr",
        "outputId": "408a5e8a-6818-4bc1-c361-17c6d4887b91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning primary community labels to nodes...\n",
            "Done in 5.4s — 317,194 nodes labeled\n",
            "\n",
            "Filtered to top-20 categories: 173,905 labeled nodes\n",
            "Label distribution:\n",
            "  Class 5: 22,914 nodes\n",
            "  Class 12: 16,623 nodes\n",
            "  Class 9: 16,363 nodes\n",
            "  Class 11: 14,782 nodes\n",
            "  Class 2: 13,967 nodes\n",
            "  Class 14: 13,873 nodes\n",
            "  Class 18: 12,292 nodes\n",
            "  Class 1: 11,471 nodes\n",
            "  Class 8: 7,718 nodes\n",
            "  Class 17: 7,245 nodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 CA-HepTh Collaboration Network"
      ],
      "metadata": {
        "id": "fTVzP_OQukTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.3  Load and build HepTh collaboration graph\n",
        "# ============================================================\n",
        "import ast\n",
        "\n",
        "meta_df = pd.read_csv(\"data/arxiv_hepth_meta.csv\")\n",
        "\n",
        "# Clean authors_list column\n",
        "def safe_parse(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    try:\n",
        "        parsed = ast.literal_eval(str(x))\n",
        "        if isinstance(parsed, list):\n",
        "            return [str(a).strip() for a in parsed if len(str(a).strip()) > 1]\n",
        "    except:\n",
        "        pass\n",
        "    return []\n",
        "\n",
        "meta_df['authors_clean'] = meta_df['authors_list'].apply(safe_parse)\n",
        "meta_df['n_authors_clean'] = meta_df['authors_clean'].apply(len)\n",
        "\n",
        "# Build co-authorship graph\n",
        "G_hepth = nx.Graph()\n",
        "for _, row in meta_df.iterrows():\n",
        "    authors = row['authors_clean']\n",
        "    if len(authors) >= 2:\n",
        "        for a1, a2 in combinations(authors, 2):\n",
        "            if G_hepth.has_edge(a1, a2):\n",
        "                G_hepth[a1][a2]['weight'] += 1\n",
        "            else:\n",
        "                G_hepth.add_edge(a1, a2, weight=1)\n",
        "\n",
        "# Use largest connected component\n",
        "components = sorted(nx.connected_components(G_hepth), key=len, reverse=True)\n",
        "G_hepth_lcc = G_hepth.subgraph(components[0]).copy()\n",
        "\n",
        "print(f\"HepTh full graph: {G_hepth.number_of_nodes():,} nodes, {G_hepth.number_of_edges():,} edges\")\n",
        "print(f\"HepTh LCC:        {G_hepth_lcc.number_of_nodes():,} nodes, {G_hepth_lcc.number_of_edges():,} edges\")\n",
        "print(f\"Components: {len(components)}, LCC covers {G_hepth_lcc.number_of_nodes()/G_hepth.number_of_nodes():.1%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTl1pHHAuhhs",
        "outputId": "e4609e70-0977-428f-8374-25b1280801b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HepTh full graph: 16,210 nodes, 27,789 edges\n",
            "HepTh LCC:        10,397 nodes, 22,243 edges\n",
            "Components: 1919, LCC covers 64.1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.4  Construct node labels for HepTh (node classification)\n",
        "# ============================================================\n",
        "# Use Louvain community detection to create pseudo-labels\n",
        "# (since there is no explicit node-level ground truth beyond subject class)\n",
        "import community.community_louvain as community_louvain\n",
        "\n",
        "partition_hepth = community_louvain.best_partition(G_hepth_lcc, random_state=SEED)\n",
        "n_comms = len(set(partition_hepth.values()))\n",
        "print(f\"Louvain found {n_comms} communities on HepTh LCC\")\n",
        "\n",
        "# Keep communities with at least 20 members for meaningful classification\n",
        "comm_sizes = Counter(partition_hepth.values())\n",
        "valid_comms = {c for c, s in comm_sizes.items() if s >= 20}\n",
        "hepth_node_labels = {n: c for n, c in partition_hepth.items() if c in valid_comms}\n",
        "\n",
        "le_hepth = LabelEncoder()\n",
        "labeled_nodes_hepth = sorted(hepth_node_labels.keys())\n",
        "encoded_labels_hepth = le_hepth.fit_transform([hepth_node_labels[n] for n in labeled_nodes_hepth])\n",
        "\n",
        "print(f\"Keeping {len(valid_comms)} communities (>=20 members): {len(hepth_node_labels):,} labeled nodes\")\n",
        "print(f\"Number of classes: {len(set(encoded_labels_hepth))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs9yzCnpuheS",
        "outputId": "097e4d85-56b9-4aa9-d924-c962a1471d7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Louvain found 72 communities on HepTh LCC\n",
            "Keeping 66 communities (>=20 members): 10,333 labeled nodes\n",
            "Number of classes: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: Structural Feature Engineering\n",
        "\n",
        "Since neither dataset has inherent node attributes, we compute structural features\n",
        "that will serve as input to GNN models and as baselines for comparison."
      ],
      "metadata": {
        "id": "Qj4drzwnurC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2.1  Compute structural features for both graphs\n",
        "# ============================================================\n",
        "def compute_structural_features(G, name=\"graph\"):\n",
        "    \"\"\"Compute structural node features for a graph.\"\"\"\n",
        "    print(f\"Computing structural features for {name}...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Work on a copy so you don't mutate the original graph\n",
        "    G = G.copy()\n",
        "    n_loops = nx.number_of_selfloops(G)\n",
        "    if n_loops > 0:\n",
        "        print(f\"  Removing {n_loops} self-loops...\")\n",
        "        G.remove_edges_from(nx.selfloop_edges(G))\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    degrees = dict(G.degree())\n",
        "    features['degree'] = degrees\n",
        "    print(\"  Degree: done\")\n",
        "\n",
        "    clustering = nx.clustering(G)\n",
        "    features['clustering'] = clustering\n",
        "    print(\"  Clustering: done\")\n",
        "\n",
        "    pagerank = nx.pagerank(G, max_iter=100)\n",
        "    features['pagerank'] = pagerank\n",
        "    print(\"  PageRank: done\")\n",
        "\n",
        "    core = nx.core_number(G)\n",
        "    features['core_number'] = core\n",
        "    print(\"  Core number: done\")\n",
        "\n",
        "    print(f\"  Total time: {time.time()-t0:.1f}s\")\n",
        "    return features\n",
        "\n",
        "\n",
        "# For Amazon — compute on full graph (all these scale well)\n",
        "amazon_features = compute_structural_features(G_amazon, \"Amazon\")\n",
        "\n",
        "# For HepTh — compute on LCC\n",
        "hepth_features = compute_structural_features(G_hepth_lcc, \"HepTh LCC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doiwleotuhcW",
        "outputId": "40debd57-ce44-4fed-cb06-8a14ef63baf4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing structural features for Amazon...\n",
            "  Degree: done\n",
            "  Clustering: done\n",
            "  PageRank: done\n",
            "  Core number: done\n",
            "  Total time: 21.8s\n",
            "Computing structural features for HepTh LCC...\n",
            "  Removing 12 self-loops...\n",
            "  Degree: done\n",
            "  Clustering: done\n",
            "  PageRank: done\n",
            "  Core number: done\n",
            "  Total time: 0.3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2.2  Build feature matrices\n",
        "# ============================================================\n",
        "def build_feature_matrix(G, features, node_list=None):\n",
        "    \"\"\"Build a numpy feature matrix from structural features.\"\"\"\n",
        "    if node_list is None:\n",
        "        node_list = sorted(G.nodes())\n",
        "\n",
        "    feat_names = ['degree', 'clustering', 'pagerank', 'core_number']\n",
        "    X = np.zeros((len(node_list), len(feat_names)))\n",
        "\n",
        "    for j, fname in enumerate(feat_names):\n",
        "        for i, node in enumerate(node_list):\n",
        "            X[i, j] = features[fname].get(node, 0.0)\n",
        "\n",
        "    # Normalize\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    return X, feat_names, node_list\n",
        "\n",
        "# Amazon feature matrix (for all nodes, ordered)\n",
        "amazon_nodes_sorted = sorted(G_amazon.nodes())\n",
        "amazon_node_to_idx = {n: i for i, n in enumerate(amazon_nodes_sorted)}\n",
        "X_amazon, feat_names, _ = build_feature_matrix(G_amazon, amazon_features, amazon_nodes_sorted)\n",
        "print(f\"Amazon feature matrix: {X_amazon.shape}\")\n",
        "\n",
        "# HepTh feature matrix\n",
        "hepth_nodes_sorted = sorted(G_hepth_lcc.nodes())\n",
        "hepth_node_to_idx = {n: i for i, n in enumerate(hepth_nodes_sorted)}\n",
        "X_hepth, _, _ = build_feature_matrix(G_hepth_lcc, hepth_features, hepth_nodes_sorted)\n",
        "print(f\"HepTh feature matrix: {X_hepth.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iluk8al-uhaZ",
        "outputId": "f2637eca-d83e-4370-b35b-7db98a968dbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon feature matrix: (334863, 4)\n",
            "HepTh feature matrix: (10397, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 3: Shallow Embeddings — DeepWalk & Node2Vec\n",
        "\n",
        "## 3.1 Theory\n",
        "\n",
        "**DeepWalk** performs uniform random walks and feeds the node sequences into Word2Vec (Skip-Gram with negative sampling). It captures community structure — nodes in the same densely-connected region tend to co-occur in walks.\n",
        "\n",
        "**Node2Vec** extends this with biased random walks:\n",
        "- **p** (return parameter): High p → less likely to backtrack → explores further\n",
        "- **q** (in-out parameter): Low q → BFS-like (structural equivalence); High q → DFS-like (homophily)\n",
        "\n",
        "Both are **unsupervised** and **transductive** — they learn fixed embeddings per node."
      ],
      "metadata": {
        "id": "rzncXVwbuzPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 DeepWalk Implementation"
      ],
      "metadata": {
        "id": "cC8tdMOgu07W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3.1  DeepWalk (uniform random walks + Word2Vec)\n",
        "# ============================================================\n",
        "def deepwalk(G, dimensions=128, walk_length=40, num_walks=10,\n",
        "             window=5, workers=4, seed=42):\n",
        "    \"\"\"\n",
        "    DeepWalk: uniform random walks + Skip-Gram.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    G : nx.Graph\n",
        "    dimensions : int — embedding dimensionality\n",
        "    walk_length : int — length of each random walk\n",
        "    num_walks : int — number of walks per node\n",
        "    window : int — Word2Vec context window size\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    embeddings : dict {node: np.array}\n",
        "    model : Word2Vec model\n",
        "    \"\"\"\n",
        "    print(f\"DeepWalk: {num_walks} walks × {walk_length} steps, dim={dimensions}\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    nodes = list(G.nodes())\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Generate random walks\n",
        "    walks = []\n",
        "    for _ in range(num_walks):\n",
        "        rng.shuffle(nodes)\n",
        "        for start_node in nodes:\n",
        "            walk = [start_node]\n",
        "            current = start_node\n",
        "            for _ in range(walk_length - 1):\n",
        "                neighbors = list(G.neighbors(current))\n",
        "                if len(neighbors) == 0:\n",
        "                    break\n",
        "                current = neighbors[rng.integers(len(neighbors))]\n",
        "                walk.append(current)\n",
        "            walks.append([str(n) for n in walk])  # Word2Vec needs strings\n",
        "\n",
        "    print(f\"  Generated {len(walks):,} walks in {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Train Word2Vec\n",
        "    t1 = time.time()\n",
        "    model = Word2Vec(\n",
        "        walks, vector_size=dimensions, window=window,\n",
        "        min_count=0, sg=1, workers=workers, seed=seed, epochs=5\n",
        "    )\n",
        "    print(f\"  Word2Vec trained in {time.time()-t1:.1f}s\")\n",
        "\n",
        "    # Extract embeddings\n",
        "    embeddings = {}\n",
        "    for node in G.nodes():\n",
        "        key = str(node)\n",
        "        if key in model.wv:\n",
        "            embeddings[node] = model.wv[key]\n",
        "\n",
        "    print(f\"  Total time: {time.time()-t0:.1f}s, embedded {len(embeddings):,} nodes\")\n",
        "    return embeddings, model"
      ],
      "metadata": {
        "id": "wWId2b74uhYt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Node2Vec Implementation"
      ],
      "metadata": {
        "id": "bhwIPSifvA2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3.2  Node2Vec (biased random walks + Word2Vec)\n",
        "# ============================================================\n",
        "def node2vec_embed(G, dimensions=128, walk_length=40, num_walks=10,\n",
        "                   p=1.0, q=1.0, window=5, workers=4, seed=42):\n",
        "    \"\"\"\n",
        "    Node2Vec with biased random walks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    p : float — return parameter (high = less backtracking)\n",
        "    q : float — in-out parameter (low = BFS-like, high = DFS-like)\n",
        "    \"\"\"\n",
        "    print(f\"Node2Vec: p={p}, q={q}, dim={dimensions}, walks={num_walks}×{walk_length}\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    node2vec = Node2VecLib(\n",
        "        G, dimensions=dimensions, walk_length=walk_length,\n",
        "        num_walks=num_walks, p=p, q=q, workers=workers, seed=seed,\n",
        "        quiet=True\n",
        "    )\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(f\"  Walks generated in {t1-t0:.1f}s\")\n",
        "\n",
        "    model = node2vec.fit(window=window, min_count=0, batch_words=4, seed=seed)\n",
        "    print(f\"  Model trained in {time.time()-t1:.1f}s\")\n",
        "\n",
        "    embeddings = {}\n",
        "    for node in G.nodes():\n",
        "        key = str(node)\n",
        "        if key in model.wv:\n",
        "            embeddings[node] = model.wv[key]\n",
        "\n",
        "    print(f\"  Total: {time.time()-t0:.1f}s, embedded {len(embeddings):,} nodes\")\n",
        "    return embeddings, model"
      ],
      "metadata": {
        "id": "lpoeBkdAuhWv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Run Shallow Embeddings on Both Datasets"
      ],
      "metadata": {
        "id": "4F7ATq9MvGJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install tqdm\n",
        "from tqdm.auto import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "wnOc6CqSOTw0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "def _random_walk(G, start, walk_length):\n",
        "    walk = [start]\n",
        "    for _ in range(walk_length - 1):\n",
        "        cur = walk[-1]\n",
        "        nbrs = list(G.neighbors(cur))\n",
        "        if not nbrs:\n",
        "            break\n",
        "        walk.append(random.choice(nbrs))\n",
        "    return walk\n",
        "\n",
        "def deepwalk_with_pbar(G, dimensions=128, walk_length=40, num_walks=10, window=5, workers=2, seed=42):\n",
        "    random.seed(seed)\n",
        "    nodes = list(G.nodes())\n",
        "\n",
        "    total = num_walks * len(nodes)\n",
        "    walks = []\n",
        "    pbar = tqdm(total=total, desc=\"Generating walks\", leave=True)\n",
        "\n",
        "    for _ in range(num_walks):\n",
        "        random.shuffle(nodes)\n",
        "        for v in nodes:\n",
        "            walks.append([str(x) for x in _random_walk(G, v, walk_length)])\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    print(\"Training Word2Vec...\")\n",
        "    model = Word2Vec(\n",
        "        sentences=walks,\n",
        "        vector_size=dimensions,\n",
        "        window=window,\n",
        "        min_count=0,\n",
        "        sg=1,\n",
        "        workers=workers\n",
        "    )\n",
        "\n",
        "    emb = {int(k) if k.isdigit() else k: model.wv[k] for k in model.wv.index_to_key}\n",
        "    return emb, model\n"
      ],
      "metadata": {
        "id": "z7m7LQsAOV94"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3.3  Run DeepWalk on both datasets\n",
        "# ============================================================\n",
        "EMB_DIM = 128\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"AMAZON — DeepWalk\")\n",
        "print(\"=\" * 60)\n",
        "amazon_dw_emb, _ = deepwalk_with_pbar(G_amazon, dimensions=EMB_DIM, walk_length=40, num_walks=10, window=5)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HEPTH — DeepWalk\")\n",
        "print(\"=\" * 60)\n",
        "hepth_dw_emb, _ = deepwalk_with_pbar(G_hepth_lcc, dimensions=EMB_DIM, walk_length=40, num_walks=10, window=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "805ffdb94dab4d67bf1548c69247e763",
            "608d88cca38e4f4ca649723bfd8d329f",
            "65eb45f75fae403ea190e24105774804",
            "e61c0dcfad474c8b8081aeb45de316f5",
            "2af4cd9a63f14f40998a86b3b8a7e2da",
            "f5cc6468dbea4aae8c3bc31c6ee38397",
            "591713b699e94ce4ab8e79be8323429b",
            "2ec30572804b4cbabdfae748b9e6b01a",
            "03353b9fd753450ba56f96f35c2e774c",
            "081783c96eea426d9d320d71e9a937d8",
            "1f4158da7a5d408b9dcb24f17ff19128"
          ]
        },
        "id": "GcSZhPzIuhU0",
        "outputId": "7c2bfb1f-f833-4c1d-cda0-e2a66f49fbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "AMAZON — DeepWalk\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating walks:   0%|          | 0/3348630 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "805ffdb94dab4d67bf1548c69247e763"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 4: Spectral Embeddings — Laplacian Eigenmaps\n",
        "\n",
        "## 4.1 Theory\n",
        "\n",
        "Laplacian Eigenmaps compute the embedding by finding the k smallest non-trivial eigenvectors\n",
        "of the graph Laplacian matrix L = D − A (or its normalized variant L_norm = D^(-½) L D^(-½)).\n",
        "\n",
        "The optimization objective is:\n",
        "$$\\min_Y \\sum_{(i,j) \\in E} w_{ij} \\|y_i - y_j\\|^2 = \\min_Y \\text{tr}(Y^T L Y)$$\n",
        "subject to Y^T D Y = I.\n",
        "\n",
        "This ensures connected nodes are close in the embedding space. The solution is given by\n",
        "the eigenvectors corresponding to the smallest non-zero eigenvalues of L.\n",
        "\n",
        "**Scalability note:** Eigendecomposition of L is O(n³) for dense methods. For the Amazon\n",
        "graph (335K nodes), this is infeasible. We use scipy's sparse eigensolver (`eigsh`) which\n",
        "works well up to ~50K nodes, and restrict Amazon spectral embeddings to a subgraph."
      ],
      "metadata": {
        "id": "5wY0FcoEvL6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4.1  Laplacian Eigenmaps implementation\n",
        "# ============================================================\n",
        "def laplacian_eigenmaps(G, dimensions=128, normalized=True):\n",
        "    \"\"\"\n",
        "    Compute Laplacian Eigenmaps for graph G.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    G : nx.Graph\n",
        "    dimensions : int — number of embedding dimensions\n",
        "    normalized : bool — use normalized Laplacian if True\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    embeddings : dict {node: np.array}\n",
        "    eigenvalues : np.array\n",
        "    \"\"\"\n",
        "    n = G.number_of_nodes()\n",
        "    print(f\"Laplacian Eigenmaps: {n:,} nodes, dim={dimensions}, normalized={normalized}\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    nodes = sorted(G.nodes())\n",
        "    node_to_idx = {n: i for i, n in enumerate(nodes)}\n",
        "\n",
        "    # Build sparse adjacency matrix\n",
        "    rows, cols, vals = [], [], []\n",
        "    for u, v, d in G.edges(data=True):\n",
        "        w = d.get('weight', 1.0)\n",
        "        i, j = node_to_idx[u], node_to_idx[v]\n",
        "        rows.extend([i, j])\n",
        "        cols.extend([j, i])\n",
        "        vals.extend([w, w])\n",
        "\n",
        "    A = sparse.csr_matrix((vals, (rows, cols)), shape=(n, n))\n",
        "    D = sparse.diags(np.array(A.sum(axis=1)).flatten())\n",
        "    L = D - A\n",
        "\n",
        "    if normalized:\n",
        "        # Normalized Laplacian: D^(-1/2) L D^(-1/2)\n",
        "        D_inv_sqrt = sparse.diags(1.0 / np.sqrt(np.maximum(np.array(A.sum(axis=1)).flatten(), 1e-10)))\n",
        "        L = D_inv_sqrt @ L @ D_inv_sqrt\n",
        "\n",
        "    # Compute smallest eigenvectors (skip the first trivial one)\n",
        "    k = min(dimensions + 1, n - 1)\n",
        "    print(f\"  Computing {k} smallest eigenvectors...\")\n",
        "    eigenvalues, eigenvectors = eigsh(L, k=k, which='SM', tol=1e-6)\n",
        "\n",
        "    # Sort by eigenvalue and skip the first (zero eigenvalue)\n",
        "    idx = np.argsort(eigenvalues)\n",
        "    eigenvalues = eigenvalues[idx]\n",
        "    eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "    # Skip first eigenvector (constant), take next 'dimensions'\n",
        "    embedding_matrix = eigenvectors[:, 1:dimensions+1]\n",
        "\n",
        "    # Build embeddings dict\n",
        "    embeddings = {}\n",
        "    for i, node in enumerate(nodes):\n",
        "        embeddings[node] = embedding_matrix[i]\n",
        "\n",
        "    print(f\"  Done in {time.time()-t0:.1f}s\")\n",
        "    print(f\"  Smallest eigenvalues: {eigenvalues[:5].round(6)}\")\n",
        "\n",
        "    return embeddings, eigenvalues"
      ],
      "metadata": {
        "id": "0A8KWmQouhTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4.2  Run spectral embeddings\n",
        "# ============================================================\n",
        "# --- HepTh (full LCC — fits in memory) ---\n",
        "print(\"=\" * 60)\n",
        "print(\"HEPTH — Laplacian Eigenmaps (full LCC)\")\n",
        "print(\"=\" * 60)\n",
        "hepth_spectral_emb, hepth_eigenvalues = laplacian_eigenmaps(\n",
        "    G_hepth_lcc, dimensions=EMB_DIM, normalized=True\n",
        ")\n",
        "\n",
        "# --- Amazon (sampled subgraph — full graph is too large) ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"AMAZON — Laplacian Eigenmaps (sampled subgraph)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# BFS sample ~15K nodes for spectral embedding\n",
        "def bfs_sample(G, n_nodes=15000, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    start = rng.choice(list(G.nodes()))\n",
        "    visited = {start}\n",
        "    q = deque([start])\n",
        "    while q and len(visited) < n_nodes:\n",
        "        u = q.popleft()\n",
        "        for v in G.neighbors(u):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                q.append(v)\n",
        "                if len(visited) >= n_nodes:\n",
        "                    break\n",
        "    return G.subgraph(visited).copy()\n",
        "\n",
        "G_amazon_sub = bfs_sample(G_amazon, n_nodes=15000, seed=SEED)\n",
        "print(f\"Amazon subgraph for spectral: {G_amazon_sub.number_of_nodes():,} nodes, {G_amazon_sub.number_of_edges():,} edges\")\n",
        "\n",
        "amazon_spectral_emb, amazon_eigenvalues = laplacian_eigenmaps(\n",
        "    G_amazon_sub, dimensions=EMB_DIM, normalized=True\n",
        ")"
      ],
      "metadata": {
        "id": "AWsWu0gEuhQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4.3  Visualize eigenvalue spectrum (spectral gap)\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# HepTh\n",
        "ax = axes[0]\n",
        "ax.plot(range(1, min(51, len(hepth_eigenvalues))),\n",
        "        hepth_eigenvalues[1:min(51, len(hepth_eigenvalues))],\n",
        "        'o-', markersize=4, color='steelblue')\n",
        "ax.set_xlabel(\"Eigenvalue index\")\n",
        "ax.set_ylabel(\"Eigenvalue\")\n",
        "ax.set_title(\"HepTh — Laplacian Eigenvalue Spectrum\")\n",
        "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Amazon subgraph\n",
        "ax = axes[1]\n",
        "ax.plot(range(1, min(51, len(amazon_eigenvalues))),\n",
        "        amazon_eigenvalues[1:min(51, len(amazon_eigenvalues))],\n",
        "        'o-', markersize=4, color='coral')\n",
        "ax.set_xlabel(\"Eigenvalue index\")\n",
        "ax.set_ylabel(\"Eigenvalue\")\n",
        "ax.set_title(\"Amazon (subgraph) — Laplacian Eigenvalue Spectrum\")\n",
        "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"eigenvalue_spectrum.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"The spectral gap (difference between 1st and 2nd smallest eigenvalues)\")\n",
        "print(\"indicates community structure strength. A larger gap = clearer communities.\")"
      ],
      "metadata": {
        "id": "Bv237l0BuhO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 5: Graph Neural Networks — GCN, GraphSAGE, GAT\n",
        "\n",
        "## 5.1 Theory Overview\n",
        "\n",
        "All GNNs follow the **message-passing** paradigm: each layer updates a node's representation\n",
        "by aggregating information from its neighbors.\n",
        "\n",
        "**GCN** (Graph Convolutional Network): Uses a fixed, symmetric normalization:\n",
        "`h_v^(l+1) = σ(Σ_{u∈N(v)∪{v}} (1/√(d_u · d_v)) · h_u^(l) · W^(l))`\n",
        "\n",
        "**GraphSAGE**: Samples a fixed number of neighbors and uses a learnable aggregator\n",
        "(mean, LSTM, or max-pool). Key advantage: **inductive** + **scalable** via mini-batching.\n",
        "\n",
        "**GAT**: Uses multi-head **attention** to learn neighbor importance weights:\n",
        "`α_{ij} = softmax(LeakyReLU(a^T [W·h_i || W·h_j]))`\n",
        "More expressive but more expensive. Multiple attention heads provide stability."
      ],
      "metadata": {
        "id": "c5_VntY9vSYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Model Definitions"
      ],
      "metadata": {
        "id": "3Yo8F1TuvU0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.1  GCN Model\n",
        "# ============================================================\n",
        "class GCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Convolutional Network for node classification.\n",
        "\n",
        "    Architecture: Input -> GCN -> ReLU -> Dropout -> GCN -> ReLU -> Dropout -> GCN -> Output\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def get_embedding(self, x, edge_index):\n",
        "        \"\"\"Return the penultimate layer embedding.\"\"\"\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Dm8J4iiVuhNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.2  GraphSAGE Model\n",
        "# ============================================================\n",
        "class GraphSAGE(nn.Module):\n",
        "    \"\"\"\n",
        "    GraphSAGE for node classification with neighbor sampling.\n",
        "\n",
        "    Uses mean aggregation — the most common and scalable variant.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def get_embedding(self, x, edge_index):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uhUMYHPRuhK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.3  GAT Model\n",
        "# ============================================================\n",
        "class GAT(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Network for node classification.\n",
        "\n",
        "    Uses multi-head attention to learn neighbor importance.\n",
        "    First layer uses 'heads' attention heads (outputs are concatenated).\n",
        "    Last layer uses 1 head (output is averaged for stability).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers=2, heads=4, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        # First layer: multi-head attention, output concatenated\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "        # Last layer: single head, average\n",
        "        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.elu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def get_embedding(self, x, edge_index):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.elu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "r2sqyjFduhJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Training Utilities"
      ],
      "metadata": {
        "id": "aDLTZV08x850"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.4  GNN Training utilities\n",
        "# ============================================================\n",
        "def train_gnn_epoch(model, data, optimizer, criterion, mask):\n",
        "    \"\"\"Train one epoch for node classification.\"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[mask], data.y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_gnn(model, data, mask):\n",
        "    \"\"\"Evaluate node classification performance.\"\"\"\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out[mask].argmax(dim=1)\n",
        "    y_true = data.y[mask].cpu().numpy()\n",
        "    y_pred = pred.cpu().numpy()\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "    return {'accuracy': acc, 'f1_macro': f1_macro, 'f1_micro': f1_micro}\n",
        "\n",
        "def train_gnn_full(model, data, epochs=200, lr=0.01, weight_decay=5e-4,\n",
        "                   patience=20, verbose=True):\n",
        "    \"\"\"\n",
        "    Full training loop with early stopping.\n",
        "\n",
        "    Returns the best model state (by validation F1) and training history.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_f1 = 0\n",
        "    best_state = None\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loss = train_gnn_epoch(model, data, optimizer, criterion, data.train_mask)\n",
        "        val_metrics = eval_gnn(model, data, data.val_mask)\n",
        "\n",
        "        history['train_loss'].append(loss)\n",
        "        history['val_acc'].append(val_metrics['accuracy'])\n",
        "        history['val_f1'].append(val_metrics['f1_macro'])\n",
        "\n",
        "        if val_metrics['f1_macro'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1_macro']\n",
        "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            if verbose:\n",
        "                print(f\"  Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "        if verbose and epoch % 50 == 0:\n",
        "            print(f\"  Epoch {epoch:3d}: loss={loss:.4f}, val_acc={val_metrics['accuracy']:.4f}, val_f1={val_metrics['f1_macro']:.4f}\")\n",
        "\n",
        "    # Restore best model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # Final test evaluation\n",
        "    test_metrics = eval_gnn(model, data, data.test_mask)\n",
        "\n",
        "    return model, history, test_metrics"
      ],
      "metadata": {
        "id": "EfAvELoTuhHP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.5  Prepare PyG Data objects\n",
        "# ============================================================\n",
        "def prepare_pyg_data(G, X_features, node_labels, labeled_nodes,\n",
        "                     node_to_idx, encoded_labels, train_ratio=0.6,\n",
        "                     val_ratio=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Convert a NetworkX graph + features + labels into a PyG Data object\n",
        "    with train/val/test masks.\n",
        "    \"\"\"\n",
        "    nodes_sorted = sorted(G.nodes())\n",
        "    n = len(nodes_sorted)\n",
        "\n",
        "    # Edge index\n",
        "    edges = list(G.edges())\n",
        "    src = [node_to_idx[u] for u, v in edges]\n",
        "    dst = [node_to_idx[v] for u, v in edges]\n",
        "    # Make undirected\n",
        "    edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n",
        "\n",
        "    # Node features\n",
        "    x = torch.tensor(X_features, dtype=torch.float)\n",
        "\n",
        "    # Labels (full array, -1 for unlabeled)\n",
        "    y = torch.full((n,), -1, dtype=torch.long)\n",
        "    labeled_indices = [node_to_idx[node] for node in labeled_nodes]\n",
        "    for idx, label in zip(labeled_indices, encoded_labels):\n",
        "        y[idx] = label\n",
        "\n",
        "    # Create masks\n",
        "    labeled_indices = np.array(labeled_indices)\n",
        "    labels_for_split = encoded_labels\n",
        "\n",
        "    idx_train, idx_temp, y_train, y_temp = train_test_split(\n",
        "        labeled_indices, labels_for_split,\n",
        "        train_size=train_ratio, stratify=labels_for_split, random_state=seed\n",
        "    )\n",
        "    val_size = val_ratio / (1 - train_ratio)\n",
        "    idx_val, idx_test, _, _ = train_test_split(\n",
        "        idx_temp, y_temp,\n",
        "        train_size=val_size, stratify=y_temp, random_state=seed\n",
        "    )\n",
        "\n",
        "    train_mask = torch.zeros(n, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(n, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(n, dtype=torch.bool)\n",
        "    train_mask[idx_train] = True\n",
        "    val_mask[idx_val] = True\n",
        "    test_mask[idx_test] = True\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y,\n",
        "                train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
        "\n",
        "    print(f\"PyG Data: {data}\")\n",
        "    print(f\"  Train: {train_mask.sum().item():,}, Val: {val_mask.sum().item():,}, Test: {test_mask.sum().item():,}\")\n",
        "    print(f\"  Num classes: {len(set(encoded_labels))}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# --- HepTh PyG Data ---\n",
        "print(\"Preparing HepTh PyG data...\")\n",
        "data_hepth = prepare_pyg_data(\n",
        "    G_hepth_lcc, X_hepth, hepth_node_labels, labeled_nodes_hepth,\n",
        "    hepth_node_to_idx, encoded_labels_hepth\n",
        ")\n",
        "data_hepth = data_hepth.to(DEVICE)\n",
        "\n",
        "# --- Amazon PyG Data ---\n",
        "print(\"\\nPreparing Amazon PyG data...\")\n",
        "data_amazon = prepare_pyg_data(\n",
        "    G_amazon, X_amazon, amazon_node_labels_filtered, labeled_nodes_amazon,\n",
        "    amazon_node_to_idx, encoded_labels_amazon\n",
        ")\n",
        "data_amazon = data_amazon.to(DEVICE)"
      ],
      "metadata": {
        "id": "JAAN8Z46uhFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 6: Node Classification Experiments\n",
        "\n",
        "We now evaluate all embedding methods on the node classification task.\n",
        "\n",
        "**Protocol:**\n",
        "1. **Shallow + Spectral embeddings → Logistic Regression**: Use the learned embeddings as\n",
        "   features and train a downstream classifier. This is the standard evaluation protocol\n",
        "   for unsupervised embeddings.\n",
        "2. **GNNs → End-to-end**: Train the GNN directly for classification (supervised).\n",
        "\n",
        "This comparison is fair because we report the same metrics (accuracy, macro-F1) on the\n",
        "same test set for all methods."
      ],
      "metadata": {
        "id": "IlL0UxxlyChA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Embedding-based Classification (Logistic Regression)"
      ],
      "metadata": {
        "id": "tWdBEiEvyED-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.1  Evaluate embeddings with Logistic Regression\n",
        "# ============================================================\n",
        "def evaluate_embedding_classification(embeddings, labeled_nodes, encoded_labels,\n",
        "                                      name=\"Embedding\", test_size=0.2, val_size=0.2):\n",
        "    \"\"\"\n",
        "    Evaluate node embeddings for classification using Logistic Regression.\n",
        "\n",
        "    Returns dict with accuracy, f1_macro, f1_micro on the test set.\n",
        "    \"\"\"\n",
        "    # Filter to nodes present in embedding\n",
        "    valid = [(n, l) for n, l in zip(labeled_nodes, encoded_labels) if n in embeddings]\n",
        "    if len(valid) == 0:\n",
        "        print(f\"  {name}: No valid nodes found!\")\n",
        "        return None\n",
        "\n",
        "    nodes, labels = zip(*valid)\n",
        "    X = np.array([embeddings[n] for n in nodes])\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(test_size + val_size), stratify=y, random_state=SEED\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=test_size/(test_size+val_size),\n",
        "        stratify=y_temp, random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Train Logistic Regression\n",
        "    clf = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial', random_state=SEED)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    results = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'f1_macro': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'f1_micro': f1_score(y_test, y_pred, average='micro', zero_division=0),\n",
        "        'n_test': len(y_test),\n",
        "    }\n",
        "\n",
        "    print(f\"  {name}: acc={results['accuracy']:.4f}, F1-macro={results['f1_macro']:.4f}, F1-micro={results['f1_micro']:.4f} (n={results['n_test']})\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "boKrHm4YuhDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.2  Node Classification — HepTh (all methods)\n",
        "# ============================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"NODE CLASSIFICATION — HepTh\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "hepth_nc_results = {}\n",
        "\n",
        "# --- Structural features baseline ---\n",
        "struct_emb_hepth = {n: X_hepth[hepth_node_to_idx[n]] for n in hepth_nodes_sorted}\n",
        "hepth_nc_results['Structural Features'] = evaluate_embedding_classification(\n",
        "    struct_emb_hepth, labeled_nodes_hepth, encoded_labels_hepth, \"Structural Features\"\n",
        ")\n",
        "\n",
        "# --- DeepWalk ---\n",
        "hepth_nc_results['DeepWalk'] = evaluate_embedding_classification(\n",
        "    hepth_dw_emb, labeled_nodes_hepth, encoded_labels_hepth, \"DeepWalk\"\n",
        ")\n",
        "\n",
        "# --- Node2Vec variants ---\n",
        "for config_name, emb in hepth_n2v_embs.items():\n",
        "    name = f\"Node2Vec ({config_name})\"\n",
        "    hepth_nc_results[name] = evaluate_embedding_classification(\n",
        "        emb, labeled_nodes_hepth, encoded_labels_hepth, name\n",
        "    )\n",
        "\n",
        "# --- Spectral ---\n",
        "hepth_nc_results['Spectral (Laplacian)'] = evaluate_embedding_classification(\n",
        "    hepth_spectral_emb, labeled_nodes_hepth, encoded_labels_hepth, \"Spectral (Laplacian)\"\n",
        ")"
      ],
      "metadata": {
        "id": "D--vV0YkuhBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.3  Node Classification — HepTh — GNNs\n",
        "# ============================================================\n",
        "num_classes_hepth = len(set(encoded_labels_hepth))\n",
        "in_channels_hepth = data_hepth.x.shape[1]\n",
        "\n",
        "print(\"\\n--- GCN ---\")\n",
        "model_gcn_hepth = GCN(in_channels_hepth, 128, num_classes_hepth, num_layers=2, dropout=0.5).to(DEVICE)\n",
        "model_gcn_hepth, hist_gcn, test_gcn = train_gnn_full(\n",
        "    model_gcn_hepth, data_hepth, epochs=300, lr=0.01, patience=30\n",
        ")\n",
        "hepth_nc_results['GCN'] = test_gcn\n",
        "print(f\"  GCN: acc={test_gcn['accuracy']:.4f}, F1-macro={test_gcn['f1_macro']:.4f}\")\n",
        "\n",
        "print(\"\\n--- GraphSAGE ---\")\n",
        "model_sage_hepth = GraphSAGE(in_channels_hepth, 128, num_classes_hepth, num_layers=2, dropout=0.5).to(DEVICE)\n",
        "model_sage_hepth, hist_sage, test_sage = train_gnn_full(\n",
        "    model_sage_hepth, data_hepth, epochs=300, lr=0.01, patience=30\n",
        ")\n",
        "hepth_nc_results['GraphSAGE'] = test_sage\n",
        "print(f\"  GraphSAGE: acc={test_sage['accuracy']:.4f}, F1-macro={test_sage['f1_macro']:.4f}\")\n",
        "\n",
        "print(\"\\n--- GAT ---\")\n",
        "model_gat_hepth = GAT(in_channels_hepth, 32, num_classes_hepth, num_layers=2, heads=4, dropout=0.5).to(DEVICE)\n",
        "model_gat_hepth, hist_gat, test_gat = train_gnn_full(\n",
        "    model_gat_hepth, data_hepth, epochs=300, lr=0.005, patience=30\n",
        ")\n",
        "hepth_nc_results['GAT'] = test_gat\n",
        "print(f\"  GAT: acc={test_gat['accuracy']:.4f}, F1-macro={test_gat['f1_macro']:.4f}\")"
      ],
      "metadata": {
        "id": "zXFVL2zaug_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.4  Node Classification — Amazon (all methods)\n",
        "# ============================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"NODE CLASSIFICATION — Amazon\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "amazon_nc_results = {}\n",
        "\n",
        "# --- Structural features baseline ---\n",
        "struct_emb_amazon = {n: X_amazon[amazon_node_to_idx[n]] for n in amazon_nodes_sorted}\n",
        "amazon_nc_results['Structural Features'] = evaluate_embedding_classification(\n",
        "    struct_emb_amazon, labeled_nodes_amazon, encoded_labels_amazon, \"Structural Features\"\n",
        ")\n",
        "\n",
        "# --- DeepWalk ---\n",
        "amazon_nc_results['DeepWalk'] = evaluate_embedding_classification(\n",
        "    amazon_dw_emb, labeled_nodes_amazon, encoded_labels_amazon, \"DeepWalk\"\n",
        ")\n",
        "\n",
        "# --- Node2Vec variants ---\n",
        "for config_name, emb in amazon_n2v_embs.items():\n",
        "    name = f\"Node2Vec ({config_name})\"\n",
        "    amazon_nc_results[name] = evaluate_embedding_classification(\n",
        "        emb, labeled_nodes_amazon, encoded_labels_amazon, name\n",
        "    )\n",
        "\n",
        "# --- Spectral (subgraph only) ---\n",
        "# Only evaluate nodes in the subgraph\n",
        "sub_labeled = [n for n in labeled_nodes_amazon if n in amazon_spectral_emb]\n",
        "sub_labels = [encoded_labels_amazon[labeled_nodes_amazon.index(n)] for n in sub_labeled]\n",
        "if len(sub_labeled) > 100:\n",
        "    amazon_nc_results['Spectral (subgraph)'] = evaluate_embedding_classification(\n",
        "        amazon_spectral_emb, sub_labeled, sub_labels, \"Spectral (subgraph)\"\n",
        "    )\n",
        "else:\n",
        "    print(\"  Spectral: too few labeled nodes in subgraph, skipping\")"
      ],
      "metadata": {
        "id": "FeuxnQA-ug-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.5  Node Classification — Amazon — GNNs\n",
        "# ============================================================\n",
        "num_classes_amazon = len(set(encoded_labels_amazon))\n",
        "in_channels_amazon = data_amazon.x.shape[1]\n",
        "\n",
        "# For the large Amazon graph, we use GraphSAGE with mini-batch training\n",
        "# GCN and GAT are run on a subgraph for comparison\n",
        "\n",
        "print(\"\\n--- GraphSAGE (full graph, mini-batch) ---\")\n",
        "model_sage_amazon = GraphSAGE(in_channels_amazon, 128, num_classes_amazon, num_layers=2, dropout=0.5).to(DEVICE)\n",
        "\n",
        "# Mini-batch training with NeighborLoader\n",
        "train_loader = NeighborLoader(\n",
        "    data_amazon.cpu(),\n",
        "    num_neighbors=[15, 10],\n",
        "    batch_size=1024,\n",
        "    input_nodes=data_amazon.train_mask.cpu(),\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model_sage_amazon.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_val_f1 = 0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    model_sage_amazon.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model_sage_amazon(batch.x, batch.edge_index)\n",
        "        # Only compute loss on seed nodes (first batch_size nodes)\n",
        "        mask = batch.train_mask[:batch.batch_size] if hasattr(batch, 'train_mask') else slice(None, batch.batch_size)\n",
        "        loss = criterion(out[:batch.batch_size], batch.y[:batch.batch_size])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validate (full-batch on CPU if needed)\n",
        "    if epoch % 10 == 0:\n",
        "        val_metrics = eval_gnn(model_sage_amazon, data_amazon.to(DEVICE), data_amazon.val_mask.to(DEVICE))\n",
        "        if val_metrics['f1_macro'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1_macro']\n",
        "            best_state = {k: v.clone() for k, v in model_sage_amazon.state_dict().items()}\n",
        "        print(f\"  Epoch {epoch}: loss={total_loss/len(train_loader):.4f}, val_F1={val_metrics['f1_macro']:.4f}\")\n",
        "\n",
        "if best_state:\n",
        "    model_sage_amazon.load_state_dict(best_state)\n",
        "test_sage_amazon = eval_gnn(model_sage_amazon, data_amazon.to(DEVICE), data_amazon.test_mask.to(DEVICE))\n",
        "amazon_nc_results['GraphSAGE'] = test_sage_amazon\n",
        "print(f\"  GraphSAGE: acc={test_sage_amazon['accuracy']:.4f}, F1-macro={test_sage_amazon['f1_macro']:.4f}\")"
      ],
      "metadata": {
        "id": "3mqa91Knug7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6.6  Amazon GCN (on subgraph for comparison)\n",
        "# ============================================================\n",
        "# Prepare subgraph PyG data\n",
        "sub_nodes = sorted(G_amazon_sub.nodes())\n",
        "sub_node_to_idx = {n: i for i, n in enumerate(sub_nodes)}\n",
        "\n",
        "# Build subgraph features\n",
        "X_sub = np.array([X_amazon[amazon_node_to_idx[n]] for n in sub_nodes])\n",
        "\n",
        "# Filter labeled nodes to subgraph\n",
        "sub_labeled_nodes = [n for n in labeled_nodes_amazon if n in sub_node_to_idx]\n",
        "sub_encoded = [encoded_labels_amazon[labeled_nodes_amazon.index(n)] for n in sub_labeled_nodes]\n",
        "\n",
        "if len(sub_labeled_nodes) > 200:\n",
        "    data_amazon_sub = prepare_pyg_data(\n",
        "        G_amazon_sub, X_sub,\n",
        "        {n: amazon_node_labels_filtered[n] for n in sub_labeled_nodes if n in amazon_node_labels_filtered},\n",
        "        sub_labeled_nodes, sub_node_to_idx, sub_encoded\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    num_classes_sub = len(set(sub_encoded))\n",
        "\n",
        "    print(\"\\n--- GCN (Amazon subgraph) ---\")\n",
        "    model_gcn_amazon = GCN(X_sub.shape[1], 128, num_classes_sub, num_layers=2, dropout=0.5).to(DEVICE)\n",
        "    model_gcn_amazon, _, test_gcn_amazon = train_gnn_full(\n",
        "        model_gcn_amazon, data_amazon_sub, epochs=300, lr=0.01, patience=30\n",
        "    )\n",
        "    amazon_nc_results['GCN (subgraph)'] = test_gcn_amazon\n",
        "    print(f\"  GCN (subgraph): acc={test_gcn_amazon['accuracy']:.4f}, F1-macro={test_gcn_amazon['f1_macro']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- GAT (Amazon subgraph) ---\")\n",
        "    model_gat_amazon = GAT(X_sub.shape[1], 32, num_classes_sub, num_layers=2, heads=4, dropout=0.5).to(DEVICE)\n",
        "    model_gat_amazon, _, test_gat_amazon = train_gnn_full(\n",
        "        model_gat_amazon, data_amazon_sub, epochs=300, lr=0.005, patience=30\n",
        "    )\n",
        "    amazon_nc_results['GAT (subgraph)'] = test_gat_amazon\n",
        "    print(f\"  GAT (subgraph): acc={test_gat_amazon['accuracy']:.4f}, F1-macro={test_gat_amazon['f1_macro']:.4f}\")\n",
        "else:\n",
        "    print(\"  Too few labeled nodes in Amazon subgraph for GCN/GAT training\")"
      ],
      "metadata": {
        "id": "NXNhqLxoyMpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 7: Link Prediction Experiments\n",
        "\n",
        "Link prediction evaluates whether an embedding can determine if two nodes should be connected.\n",
        "\n",
        "**Protocol:**\n",
        "1. **Edge split**: Remove a fraction of edges as positive test examples. Sample an equal\n",
        "   number of non-edges as negative test examples.\n",
        "2. **For shallow/spectral embeddings**: Compute edge scores using dot product, cosine\n",
        "   similarity, or Hadamard product of node embeddings → Logistic Regression.\n",
        "3. **For GNNs**: Train a link-prediction-specific model or use embedding similarity.\n",
        "\n",
        "**Metrics**: AUC-ROC and Average Precision (AP)."
      ],
      "metadata": {
        "id": "jEyZ2g8uyO1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.1  Link prediction utilities\n",
        "# ============================================================\n",
        "def prepare_link_prediction_data(G, test_ratio=0.1, val_ratio=0.05, seed=42):\n",
        "    \"\"\"\n",
        "    Split edges into train/val/test for link prediction.\n",
        "\n",
        "    Returns:\n",
        "    - G_train: graph with test+val edges removed\n",
        "    - pos_val_edges, neg_val_edges\n",
        "    - pos_test_edges, neg_test_edges\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    edges = list(G.edges())\n",
        "    rng.shuffle(edges)\n",
        "\n",
        "    n_test = int(len(edges) * test_ratio)\n",
        "    n_val = int(len(edges) * val_ratio)\n",
        "\n",
        "    test_edges = edges[:n_test]\n",
        "    val_edges = edges[n_test:n_test + n_val]\n",
        "    train_edges = edges[n_test + n_val:]\n",
        "\n",
        "    # Build training graph\n",
        "    G_train = nx.Graph()\n",
        "    G_train.add_nodes_from(G.nodes())\n",
        "    G_train.add_edges_from(train_edges)\n",
        "\n",
        "    # Ensure connectivity of training graph (add back critical edges)\n",
        "    # For simplicity, just use the edges as-is\n",
        "\n",
        "    # Sample negative edges\n",
        "    nodes = list(G.nodes())\n",
        "    non_edges_set = set()\n",
        "    existing_edges_set = set(G.edges()) | set((v, u) for u, v in G.edges())\n",
        "\n",
        "    while len(non_edges_set) < n_test + n_val:\n",
        "        u = nodes[rng.integers(len(nodes))]\n",
        "        v = nodes[rng.integers(len(nodes))]\n",
        "        if u != v and (u, v) not in existing_edges_set and (u, v) not in non_edges_set:\n",
        "            non_edges_set.add((u, v))\n",
        "\n",
        "    non_edges = list(non_edges_set)\n",
        "    neg_test_edges = non_edges[:n_test]\n",
        "    neg_val_edges = non_edges[n_test:n_test + n_val]\n",
        "\n",
        "    print(f\"Link prediction split:\")\n",
        "    print(f\"  Train edges: {len(train_edges):,}\")\n",
        "    print(f\"  Val edges: {len(val_edges):,} pos + {len(neg_val_edges):,} neg\")\n",
        "    print(f\"  Test edges: {len(test_edges):,} pos + {len(neg_test_edges):,} neg\")\n",
        "\n",
        "    return G_train, val_edges, neg_val_edges, test_edges, neg_test_edges\n",
        "\n",
        "def evaluate_link_prediction(embeddings, pos_edges, neg_edges, method='dot', name=\"\"):\n",
        "    \"\"\"\n",
        "    Evaluate link prediction using embedding similarity.\n",
        "\n",
        "    Methods: 'dot' (dot product), 'cosine', 'hadamard' (+ LR)\n",
        "    \"\"\"\n",
        "    def get_scores(edges, embeddings, method):\n",
        "        scores = []\n",
        "        valid = 0\n",
        "        for u, v in edges:\n",
        "            if u in embeddings and v in embeddings:\n",
        "                eu, ev = embeddings[u], embeddings[v]\n",
        "                if method == 'dot':\n",
        "                    scores.append(np.dot(eu, ev))\n",
        "                elif method == 'cosine':\n",
        "                    norm = np.linalg.norm(eu) * np.linalg.norm(ev)\n",
        "                    scores.append(np.dot(eu, ev) / max(norm, 1e-10))\n",
        "                valid += 1\n",
        "            else:\n",
        "                scores.append(0.0)\n",
        "        return np.array(scores), valid\n",
        "\n",
        "    pos_scores, n_pos = get_scores(pos_edges, embeddings, method)\n",
        "    neg_scores, n_neg = get_scores(neg_edges, embeddings, method)\n",
        "\n",
        "    y_true = np.concatenate([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
        "    y_scores = np.concatenate([pos_scores, neg_scores])\n",
        "\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "\n",
        "    print(f\"  {name} ({method}): AUC={auc:.4f}, AP={ap:.4f} (pos={n_pos}, neg={n_neg})\")\n",
        "    return {'auc': auc, 'ap': ap, 'method': method}"
      ],
      "metadata": {
        "id": "ZcSs-iH0yMnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.2  Link Prediction — HepTh\n",
        "# ============================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"LINK PREDICTION — HepTh\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Prepare edge split\n",
        "G_hepth_train, val_e, neg_val_e, test_e, neg_test_e = prepare_link_prediction_data(\n",
        "    G_hepth_lcc, test_ratio=0.1, val_ratio=0.05\n",
        ")\n",
        "\n",
        "# Re-train embeddings on the training graph (no data leakage!)\n",
        "print(\"\\nRetraining embeddings on training graph...\")\n",
        "\n",
        "hepth_lp_dw, _ = deepwalk(G_hepth_train, dimensions=EMB_DIM, walk_length=40, num_walks=10)\n",
        "hepth_lp_n2v, _ = node2vec_embed(G_hepth_train, dimensions=EMB_DIM, walk_length=40, num_walks=10, p=1.0, q=0.5)\n",
        "hepth_lp_spectral, _ = laplacian_eigenmaps(G_hepth_train, dimensions=EMB_DIM, normalized=True)\n",
        "\n",
        "hepth_lp_results = {}\n",
        "\n",
        "print(\"\\nEvaluating on test edges:\")\n",
        "for emb_name, emb in [\n",
        "    (\"DeepWalk\", hepth_lp_dw),\n",
        "    (\"Node2Vec (BFS)\", hepth_lp_n2v),\n",
        "    (\"Spectral\", hepth_lp_spectral),\n",
        "]:\n",
        "    for method in ['dot', 'cosine']:\n",
        "        key = f\"{emb_name} ({method})\"\n",
        "        hepth_lp_results[key] = evaluate_link_prediction(emb, test_e, neg_test_e, method=method, name=emb_name)"
      ],
      "metadata": {
        "id": "_G_na5dHyMlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.3  Link Prediction — HepTh — GNN-based\n",
        "# ============================================================\n",
        "print(\"\\n--- GNN Link Prediction (HepTh) ---\")\n",
        "\n",
        "class LinkPredictor(nn.Module):\n",
        "    \"\"\"Simple MLP link predictor on top of GNN embeddings.\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(2 * in_channels, in_channels)\n",
        "        self.lin2 = nn.Linear(in_channels, 1)\n",
        "\n",
        "    def forward(self, z_u, z_v):\n",
        "        x = torch.cat([z_u, z_v], dim=-1)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return self.lin2(x).squeeze(-1)\n",
        "\n",
        "class GNNLinkPredictor(nn.Module):\n",
        "    \"\"\"GNN encoder + Link predictor.\"\"\"\n",
        "    def __init__(self, encoder, predictor):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.predictor = predictor\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        return self.encoder.get_embedding(x, edge_index)\n",
        "\n",
        "    def predict(self, z, edge_label_index):\n",
        "        return self.predictor(z[edge_label_index[0]], z[edge_label_index[1]])\n",
        "\n",
        "# Prepare PyG data for link prediction (training graph)\n",
        "hepth_train_nodes = sorted(G_hepth_train.nodes())\n",
        "hepth_train_n2i = {n: i for i, n in enumerate(hepth_train_nodes)}\n",
        "\n",
        "train_src = [hepth_train_n2i[u] for u, v in G_hepth_train.edges()]\n",
        "train_dst = [hepth_train_n2i[v] for u, v in G_hepth_train.edges()]\n",
        "train_edge_index = torch.tensor([train_src + train_dst, train_dst + train_src], dtype=torch.long)\n",
        "\n",
        "# Features\n",
        "X_hepth_train = np.array([X_hepth[hepth_node_to_idx[n]] if n in hepth_node_to_idx else np.zeros(X_hepth.shape[1])\n",
        "                           for n in hepth_train_nodes])\n",
        "x_train = torch.tensor(X_hepth_train, dtype=torch.float).to(DEVICE)\n",
        "train_edge_index = train_edge_index.to(DEVICE)\n",
        "\n",
        "# Positive and negative test edges as indices\n",
        "def edges_to_index(edges, n2i):\n",
        "    valid_edges = [(n2i[u], n2i[v]) for u, v in edges if u in n2i and v in n2i]\n",
        "    if len(valid_edges) == 0:\n",
        "        return torch.zeros((2, 0), dtype=torch.long)\n",
        "    src, dst = zip(*valid_edges)\n",
        "    return torch.tensor([src, dst], dtype=torch.long)\n",
        "\n",
        "pos_test_idx = edges_to_index(test_e, hepth_train_n2i).to(DEVICE)\n",
        "neg_test_idx = edges_to_index(neg_test_e, hepth_train_n2i).to(DEVICE)\n",
        "\n",
        "# Train GNN for link prediction\n",
        "encoder = GraphSAGE(X_hepth_train.shape[1], 128, 64, num_layers=2, dropout=0.3).to(DEVICE)\n",
        "predictor = LinkPredictor(64).to(DEVICE)\n",
        "lp_model = GNNLinkPredictor(encoder, predictor).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(lp_model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 201):\n",
        "    lp_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    z = lp_model.encode(x_train, train_edge_index)\n",
        "\n",
        "    # Positive edges (sample from training)\n",
        "    pos_idx = train_edge_index[:, :len(train_src)]  # use first half (undirected)\n",
        "    pos_pred = lp_model.predict(z, pos_idx)\n",
        "\n",
        "    # Negative sampling\n",
        "    neg_idx = negative_sampling(train_edge_index, num_nodes=len(hepth_train_nodes),\n",
        "                                num_neg_samples=pos_idx.shape[1])\n",
        "    neg_pred = lp_model.predict(z, neg_idx)\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(\n",
        "        torch.cat([pos_pred, neg_pred]),\n",
        "        torch.cat([torch.ones_like(pos_pred), torch.zeros_like(neg_pred)])\n",
        "    )\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"  Epoch {epoch}: loss={loss.item():.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "lp_model.eval()\n",
        "with torch.no_grad():\n",
        "    z = lp_model.encode(x_train, train_edge_index)\n",
        "    pos_scores = torch.sigmoid(lp_model.predict(z, pos_test_idx)).cpu().numpy()\n",
        "    neg_scores = torch.sigmoid(lp_model.predict(z, neg_test_idx)).cpu().numpy()\n",
        "\n",
        "y_true = np.concatenate([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
        "y_scores = np.concatenate([pos_scores, neg_scores])\n",
        "\n",
        "gnn_auc = roc_auc_score(y_true, y_scores)\n",
        "gnn_ap = average_precision_score(y_true, y_scores)\n",
        "print(f\"\\n  GraphSAGE LP: AUC={gnn_auc:.4f}, AP={gnn_ap:.4f}\")\n",
        "hepth_lp_results['GraphSAGE (GNN)'] = {'auc': gnn_auc, 'ap': gnn_ap}"
      ],
      "metadata": {
        "id": "XgVcpFzvyMkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7.4  Link Prediction — Amazon\n",
        "# ============================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"LINK PREDICTION — Amazon\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "G_amazon_train, amz_val_e, amz_neg_val_e, amz_test_e, amz_neg_test_e = prepare_link_prediction_data(\n",
        "    G_amazon, test_ratio=0.05, val_ratio=0.02  # smaller ratios for large graph\n",
        ")\n",
        "\n",
        "# Re-train embeddings on training graph\n",
        "print(\"\\nRetraining DeepWalk on Amazon training graph...\")\n",
        "amazon_lp_dw, _ = deepwalk(G_amazon_train, dimensions=EMB_DIM, walk_length=40, num_walks=10)\n",
        "\n",
        "print(\"\\nRetraining Node2Vec (BFS-like) on Amazon training graph...\")\n",
        "amazon_lp_n2v, _ = node2vec_embed(G_amazon_train, dimensions=EMB_DIM, walk_length=40, num_walks=10, p=1.0, q=0.5)\n",
        "\n",
        "amazon_lp_results = {}\n",
        "\n",
        "print(\"\\nEvaluating on test edges:\")\n",
        "for emb_name, emb in [\n",
        "    (\"DeepWalk\", amazon_lp_dw),\n",
        "    (\"Node2Vec (BFS)\", amazon_lp_n2v),\n",
        "]:\n",
        "    for method in ['dot', 'cosine']:\n",
        "        key = f\"{emb_name} ({method})\"\n",
        "        amazon_lp_results[key] = evaluate_link_prediction(emb, amz_test_e, amz_neg_test_e, method=method, name=emb_name)"
      ],
      "metadata": {
        "id": "PKuSsGIXyMh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "---\n",
        "# Part 8: Results Comparison & Analysis"
      ],
      "metadata": {
        "id": "o5UbBe3EyMek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8.1  Summary tables\n",
        "# ============================================================\n",
        "def results_to_df(results_dict, task=\"NC\"):\n",
        "    \"\"\"Convert results dict to a pretty DataFrame.\"\"\"\n",
        "    rows = []\n",
        "    for method, metrics in results_dict.items():\n",
        "        if metrics is None:\n",
        "            continue\n",
        "        row = {'Method': method}\n",
        "        row.update(metrics)\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# --- Node Classification ---\n",
        "print(\"=\" * 70)\n",
        "print(\"NODE CLASSIFICATION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n--- HepTh ---\")\n",
        "df_nc_hepth = results_to_df(hepth_nc_results)\n",
        "if 'n_test' in df_nc_hepth.columns:\n",
        "    df_nc_hepth = df_nc_hepth.drop(columns=['n_test'])\n",
        "df_nc_hepth = df_nc_hepth.sort_values('f1_macro', ascending=False)\n",
        "print(df_nc_hepth.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "print(\"\\n--- Amazon ---\")\n",
        "df_nc_amazon = results_to_df(amazon_nc_results)\n",
        "if 'n_test' in df_nc_amazon.columns:\n",
        "    df_nc_amazon = df_nc_amazon.drop(columns=['n_test'])\n",
        "df_nc_amazon = df_nc_amazon.sort_values('f1_macro', ascending=False)\n",
        "print(df_nc_amazon.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# --- Link Prediction ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LINK PREDICTION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n--- HepTh ---\")\n",
        "df_lp_hepth = results_to_df(hepth_lp_results)\n",
        "if 'method' in df_lp_hepth.columns:\n",
        "    df_lp_hepth = df_lp_hepth.drop(columns=['method'])\n",
        "df_lp_hepth = df_lp_hepth.sort_values('auc', ascending=False)\n",
        "print(df_lp_hepth.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "print(\"\\n--- Amazon ---\")\n",
        "df_lp_amazon = results_to_df(amazon_lp_results)\n",
        "if 'method' in df_lp_amazon.columns:\n",
        "    df_lp_amazon = df_lp_amazon.drop(columns=['method'])\n",
        "df_lp_amazon = df_lp_amazon.sort_values('auc', ascending=False)\n",
        "print(df_lp_amazon.to_string(index=False, float_format='%.4f'))"
      ],
      "metadata": {
        "id": "aymQmDhByMc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8.2  Visualization — Node Classification comparison bar chart\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# HepTh\n",
        "ax = axes[0]\n",
        "if not df_nc_hepth.empty:\n",
        "    methods = df_nc_hepth['Method'].values\n",
        "    x = np.arange(len(methods))\n",
        "    width = 0.35\n",
        "    ax.bar(x - width/2, df_nc_hepth['accuracy'].values, width, label='Accuracy', color='steelblue', alpha=0.8)\n",
        "    ax.bar(x + width/2, df_nc_hepth['f1_macro'].values, width, label='F1-macro', color='coral', alpha=0.8)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Node Classification — HepTh', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 1.05)\n",
        "\n",
        "# Amazon\n",
        "ax = axes[1]\n",
        "if not df_nc_amazon.empty:\n",
        "    methods = df_nc_amazon['Method'].values\n",
        "    x = np.arange(len(methods))\n",
        "    ax.bar(x - width/2, df_nc_amazon['accuracy'].values, width, label='Accuracy', color='steelblue', alpha=0.8)\n",
        "    ax.bar(x + width/2, df_nc_amazon['f1_macro'].values, width, label='F1-macro', color='coral', alpha=0.8)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Node Classification — Amazon', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"node_classification_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hJtaP90MyMao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8.3  Visualization — Link Prediction comparison\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# HepTh LP\n",
        "ax = axes[0]\n",
        "if not df_lp_hepth.empty:\n",
        "    methods = df_lp_hepth['Method'].values\n",
        "    x = np.arange(len(methods))\n",
        "    width = 0.35\n",
        "    ax.bar(x - width/2, df_lp_hepth['auc'].values, width, label='AUC-ROC', color='steelblue', alpha=0.8)\n",
        "    ax.bar(x + width/2, df_lp_hepth['ap'].values, width, label='Avg Precision', color='coral', alpha=0.8)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Link Prediction — HepTh', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 1.05)\n",
        "\n",
        "# Amazon LP\n",
        "ax = axes[1]\n",
        "if not df_lp_amazon.empty:\n",
        "    methods = df_lp_amazon['Method'].values\n",
        "    x = np.arange(len(methods))\n",
        "    ax.bar(x - width/2, df_lp_amazon['auc'].values, width, label='AUC-ROC', color='steelblue', alpha=0.8)\n",
        "    ax.bar(x + width/2, df_lp_amazon['ap'].values, width, label='Avg Precision', color='coral', alpha=0.8)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Link Prediction — Amazon', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"link_prediction_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BmM7cQHIyMYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 9: Embedding Visualization\n",
        "\n",
        "We use UMAP to project high-dimensional embeddings to 2D, colored by class label.\n",
        "This gives an intuitive view of how well each method separates communities."
      ],
      "metadata": {
        "id": "OfFRYl9nydP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9.1  UMAP Visualization of embeddings\n",
        "# ============================================================\n",
        "def plot_embedding_umap(embeddings, labeled_nodes, labels, title=\"\", ax=None, max_points=5000):\n",
        "    \"\"\"\n",
        "    Project embeddings to 2D with UMAP and plot, colored by label.\n",
        "    \"\"\"\n",
        "    # Filter to labeled nodes present in embeddings\n",
        "    valid = [(n, l) for n, l in zip(labeled_nodes, labels) if n in embeddings]\n",
        "    if len(valid) == 0:\n",
        "        return\n",
        "\n",
        "    # Subsample if too many\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    if len(valid) > max_points:\n",
        "        idx = rng.choice(len(valid), size=max_points, replace=False)\n",
        "        valid = [valid[i] for i in idx]\n",
        "\n",
        "    nodes, labs = zip(*valid)\n",
        "    X = np.array([embeddings[n] for n in nodes])\n",
        "    y = np.array(labs)\n",
        "\n",
        "    # UMAP\n",
        "    reducer = umap.UMAP(n_components=2, random_state=SEED, n_neighbors=15, min_dist=0.1)\n",
        "    X_2d = reducer.fit_transform(X)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    scatter = ax.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab20', s=5, alpha=0.6)\n",
        "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "hMARIjtbyMRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9.2  Visualize HepTh embeddings\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "embedding_configs = [\n",
        "    (\"DeepWalk\", hepth_dw_emb),\n",
        "    (\"Node2Vec (BFS)\", hepth_n2v_embs.get(\"BFS-like\", {})),\n",
        "    (\"Node2Vec (DFS)\", hepth_n2v_embs.get(\"DFS-like\", {})),\n",
        "    (\"Spectral\", hepth_spectral_emb),\n",
        "    (\"Node2Vec (Uniform)\", hepth_n2v_embs.get(\"Uniform\", {})),\n",
        "]\n",
        "\n",
        "for idx, (name, emb) in enumerate(embedding_configs):\n",
        "    row, col = divmod(idx, 3)\n",
        "    if emb:\n",
        "        plot_embedding_umap(emb, labeled_nodes_hepth, encoded_labels_hepth,\n",
        "                           title=f\"HepTh — {name}\", ax=axes[row][col])\n",
        "    else:\n",
        "        axes[row][col].text(0.5, 0.5, \"N/A\", ha='center', va='center')\n",
        "        axes[row][col].set_title(f\"HepTh — {name}\")\n",
        "\n",
        "# GNN embedding (extract from trained model)\n",
        "model_sage_hepth.eval()\n",
        "with torch.no_grad():\n",
        "    gnn_emb_tensor = model_sage_hepth.get_embedding(data_hepth.x, data_hepth.edge_index)\n",
        "    gnn_emb_np = gnn_emb_tensor.cpu().numpy()\n",
        "\n",
        "gnn_emb_dict = {hepth_nodes_sorted[i]: gnn_emb_np[i] for i in range(len(hepth_nodes_sorted))}\n",
        "plot_embedding_umap(gnn_emb_dict, labeled_nodes_hepth, encoded_labels_hepth,\n",
        "                   title=\"HepTh — GraphSAGE (GNN)\", ax=axes[1][2])\n",
        "\n",
        "plt.suptitle(\"UMAP Projections of Node Embeddings (HepTh)\", fontsize=14, fontweight='bold', y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"hepth_embedding_umap.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "srB4FWd0yMPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9.3  Visualize Amazon embeddings (sampled)\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "amazon_emb_configs = [\n",
        "    (\"DeepWalk\", amazon_dw_emb),\n",
        "    (\"Node2Vec (BFS)\", amazon_n2v_embs.get(\"BFS-like\", {})),\n",
        "    (\"Spectral (subgraph)\", amazon_spectral_emb),\n",
        "]\n",
        "\n",
        "for idx, (name, emb) in enumerate(amazon_emb_configs):\n",
        "    if emb:\n",
        "        plot_embedding_umap(emb, labeled_nodes_amazon, encoded_labels_amazon,\n",
        "                           title=f\"Amazon — {name}\", ax=axes[idx], max_points=8000)\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, \"N/A\", ha='center', va='center')\n",
        "        axes[idx].set_title(f\"Amazon — {name}\")\n",
        "\n",
        "plt.suptitle(\"UMAP Projections of Node Embeddings (Amazon)\", fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"amazon_embedding_umap.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gKDPHI9kyMNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 10: GNN Training Analysis"
      ],
      "metadata": {
        "id": "Ehm3101_yidd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 10.1  Plot GNN training curves (HepTh)\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "histories = {\n",
        "    'GCN': hist_gcn,\n",
        "    'GraphSAGE': hist_sage,\n",
        "    'GAT': hist_gat,\n",
        "}\n",
        "\n",
        "for idx, (name, hist) in enumerate(histories.items()):\n",
        "    ax = axes[idx]\n",
        "    epochs = range(1, len(hist['train_loss']) + 1)\n",
        "\n",
        "    ax2 = ax.twinx()\n",
        "    ax.plot(epochs, hist['train_loss'], 'b-', alpha=0.6, label='Train Loss')\n",
        "    ax2.plot(epochs, hist['val_f1'], 'r-', alpha=0.8, label='Val F1-macro')\n",
        "    ax2.plot(epochs, hist['val_acc'], 'g--', alpha=0.6, label='Val Accuracy')\n",
        "\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss', color='blue')\n",
        "    ax2.set_ylabel('Score', color='red')\n",
        "    ax.set_title(f'{name} Training Curves (HepTh)', fontweight='bold')\n",
        "\n",
        "    lines1, labels1 = ax.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax.legend(lines1 + lines2, labels1 + labels2, loc='center right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"gnn_training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2IlyGkYuygpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPWj_HegyML1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rceGFEnOyMKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NC7bLOsZyMIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbkpuW4WyMGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}